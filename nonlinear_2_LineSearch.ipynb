{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Método Newton\n",
    "\n",
    "O método de Newton obtém a solução aproxima do sistema de equações não lineares através do seguinte\n",
    "processo iterativo:\n",
    "\n",
    "**Para** $k ← 0$ **ate** $k_{max}$\n",
    " 1. $\n",
    "\\mathbf{J}(\\mathbf{x}^{(k)})\\mathbf{s}= - \\mathbf{F}(\\mathbf{x}^{(k)}) \\quad \\quad (1)$\n",
    " 2. $\\mathbf{x}^{(k+1)} = \\mathbf{x}^k + \\mathbf{\\Delta x}$\n",
    " 3. **Se** $\\|F(\\mathbf{x}^{(k+1)})| < ϵ $ **E** $\\|\\Delta x\\| < ϵ$\n",
    "    - **Fim**\n",
    "\n",
    "\n",
    "onde $\\mathbf{J}$ é a matriz jacobiana dada por:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}(\\mathbf{x})^{(k)}_{ij} = \\frac{\\partial f_i(\\mathbf{x}^{(k)})}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "**Exemplo**:\n",
    "\n",
    "Resolva o sistema linear abaixo:\n",
    "\\begin{align}\n",
    "(1/b)e^{bx_0} - x_1 &= 0 \\\\\n",
    "x_0^2 + x_1^2 - 1   &= 0\n",
    "\\end{align}\n",
    "considerando $b=2$ e $\\mathbf{x}^{0} = (1,1)$.\n",
    "\n",
    "*Solução:*"
   ],
   "metadata": {
    "id": "7dPHHQdRWeCm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ComputeResiduo(x):\n",
    "    F = np.zeros([2, 1])\n",
    "    b = 2.0\n",
    "    F[0] = (1.0 / b) * np.exp(b * x[0]) - x[1]\n",
    "    F[1] = x[0] ** 2 + x[1] ** 2 - 1\n",
    "    return F\n",
    "\n",
    "\n",
    "def ComputeExJacobian(x):\n",
    "    J = np.zeros([2, 2])\n",
    "    b = 2.0\n",
    "    J[0][0] = np.exp(b * x[0])\n",
    "    J[0][1] = -1\n",
    "    J[1][0] = 2 * x[0]\n",
    "    J[1][1] = 2 * x[1]\n",
    "    return J\n",
    "\n",
    "\n",
    "def Newton(x0, kmax, tolerance):\n",
    "    print(\"%-5s  %-10.8s %-10.8s %-8.8s\" % (\"Iter\", \"x1\", \"x2\", \"error\"))\n",
    "    for i in range(kmax):\n",
    "        J = ComputeExJacobian(x0)\n",
    "        F = ComputeResiduo(x0)\n",
    "        dx = np.linalg.solve(J, -F)\n",
    "        x = x0 + dx\n",
    "        error = np.linalg.norm(x - x0) / np.linalg.norm(x)\n",
    "        print(\"%-5d  %-8.8f %-8.8f %-8.8E\" % (i, x[0], x[1], error))\n",
    "        if error < tolerance:\n",
    "            break\n",
    "        x0 = x\n",
    "    return x\n",
    "\n",
    "\n",
    "x0 = np.ones([2, 1])\n",
    "x = Newton(x0, 100, 1.0e-12)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrlnWcQ1Ye0t",
    "outputId": "d5a86ee8-3f3d-4aa5-f5f4-632c68a3bac8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter   x1         x2         error   \n",
      "0      0.61920292 0.88079708 3.70604662E-01\n",
      "1      0.39415744 0.94862308 2.28808918E-01\n",
      "2      0.32519914 0.94815663 6.87964976E-02\n",
      "3      0.31966506 0.94754696 5.56747364E-03\n",
      "4      0.31963154 0.94754191 3.38954819E-05\n",
      "5      0.31963154 0.94754191 1.24036516E-09\n",
      "6      0.31963154 0.94754191 0.00000000E+00\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Claramente a iteração de Newton requer o calculo da matriz Jacobiana, mas nós podemos fornecer apenas uma função\n",
    "F(x). As entradas do Jacobiano são, no entanto, derivadas que podem ser aproximadas por diferenças finitas. Seja $\\mathbf{u}_j\\in \\mathbb{R}^n$ um vetor com entrada 1 na posição $j$ e zero nas demais. Se $\\delta$ uma valor pqueno diferente de zero, então uma entrada da matriz $\\mathbf{J}$ pode ser aproximada por  \n",
    "\n",
    "$$\n",
    "\\mathbf{J}(\\mathbf{x})^{(k)}_{ij} \\approx  \\frac{F_i(\\mathbf{x}^{(k)} + \\delta \\mathbf{u}_j)- F_i(\\mathbf{x}^{(k)})}{\\delta }\n",
    "$$\n",
    "\n",
    "Vejamos o que acontece com e exemplo anterior:\n"
   ],
   "metadata": {
    "id": "n4xUs07_bzR7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def ComputeApproxJacobian(x):\n",
    "    eps = 1.0e-4\n",
    "    J = np.matrix([2, 2])\n",
    "    u = np.zeros([2, 1])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            u[j] = eps\n",
    "            J[i][j] = (ComputeResiduo(x + u) - ComputeResiduo(x)) / eps\n",
    "            u[j] = 0.0\n",
    "    return J\n",
    "\n",
    "\n",
    "def NewtonApprox(x0, kmax, tolerance):\n",
    "    print(\"%-5s  %-10.8s %-10.8s %-8.8s\" % (\"Iter\", \"x1\", \"x2\", \"error\"))\n",
    "    for i in range(kmax):\n",
    "        J = ComputeApproxJacobian(x0)\n",
    "        F = ComputeResiduo(x0)\n",
    "        dx = np.linalg.solve(J, -F)\n",
    "        x = x0 + dx\n",
    "        error = np.linalg.norm(x - x0) / np.linalg.norm(x)\n",
    "        print(\"%-5d  %-8.8f %-8.8f %-8.8E\" % (i, x[0], x[1], error))\n",
    "        if error < tolerance:\n",
    "            break\n",
    "        x0 = x\n",
    "    return x\n",
    "\n",
    "\n",
    "x0 = np.ones([2, 1])\n",
    "x = Newton(x0, 100, 1.0e-12)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNiHUZd0fYAR",
    "outputId": "e0d01533-f04c-4dda-fbbd-4ab7e80104d4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter   x1         x2         error   \n",
      "0      0.61920292 0.88079708 3.70604662E-01\n",
      "1      0.39415744 0.94862308 2.28808918E-01\n",
      "2      0.32519914 0.94815663 6.87964976E-02\n",
      "3      0.31966506 0.94754696 5.56747364E-03\n",
      "4      0.31963154 0.94754191 3.38954819E-05\n",
      "5      0.31963154 0.94754191 1.24036516E-09\n",
      "6      0.31963154 0.94754191 0.00000000E+00\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Método de Broyden\n",
    "\n",
    "A avaliação da matriz jacobiana em cada iterações não linear pode demandar alto custo computacional.\n",
    "O método de Broyden aproxima o jacobiano com a seguinte fórmula:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}^{k+1} = \\mathbf{B}^{k} + \\frac{(\\mathbf{y} - \\mathbf{B}^k)\\mathbf{s}^T }{\\mathbf{s}^T\\mathbf{s}} = \\mathbf{B}^{k} + \\frac{\\mathbf{F}(\\mathbf{x}^{k+1})\\mathbf{s}^T }{\\mathbf{s}^T\\mathbf{s}} \\quad k=1,2,3,...\n",
    "$$\n",
    "\n",
    "onde $\\mathbf{y} = \\mathbf{F}(\\mathbf{x}^{k+1}) - \\mathbf{F}(\\mathbf{x}^{k})$ e $\\mathbf{B}^0 = \\mathbf{J}(\\mathbf{x}^{0})$.\n",
    "\n",
    "Abaixo, segue o algoritmos em python:"
   ],
   "metadata": {
    "id": "fUBnA4XXp6wT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def broyden(x0, kmax, tolerance):\n",
    "    B = ComputeExJacobian(x0)\n",
    "    F0 = ComputeResiduo(x0)\n",
    "    for i in range(kmax):\n",
    "        dx = np.linalg.solve(B, -F0)\n",
    "        x = x0 + dx\n",
    "        error = np.linalg.norm(x - x0) / np.linalg.norm(x)\n",
    "        print(\"%-5d  %-8.8f %-8.8f %-8.8E\" % (i, x[0], x[1], error))\n",
    "        if error < tolerance:\n",
    "            break\n",
    "        F = ComputeResiduo(x)\n",
    "        r = np.dot(dx.transpose(), dx)\n",
    "        ## s^t . s\n",
    "        t = np.dot(F, dx.transpose())\n",
    "        B = B + t * (1.0 / r)\n",
    "        x0 = x\n",
    "        F0 = F\n",
    "\n",
    "\n",
    "x = broyden(x0, 100, 1.0e-12)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFAwUFpJjPOU",
    "outputId": "3613f782-76c7-43de-e4ff-4bfdcb2d7976"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0      0.61920292 0.88079708 3.70604662E-01\n",
      "1      0.47419484 0.92098311 1.45259902E-01\n",
      "2      0.36036933 0.94981664 1.15584887E-01\n",
      "3      0.32682355 0.94872605 3.34483927E-02\n",
      "4      0.32004103 0.94762896 6.86921604E-03\n",
      "5      0.31963695 0.94754343 4.13027290E-04\n",
      "6      0.31963158 0.94754193 5.57625518E-06\n",
      "7      0.31963154 0.94754192 4.37597659E-08\n",
      "8      0.31963154 0.94754191 7.56526367E-10\n",
      "9      0.31963154 0.94754191 1.34199830E-12\n",
      "10     0.31963154 0.94754191 0.00000000E+00\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estratégia de Convergência Global\n",
    "## Busca Linear (Line Search)\n",
    "\n",
    "O passo de Newton $\\mathbf{\\Delta x}$ calculado para resolver um sistema de equações não lineares pode nos levar a uma aproximação $\\mathbf{x}^{(k+1)}=\\mathbf{x}^k+\\mathbf{\\Delta x}$ indesejável onde a norma do resíduo $\\| \\mathbf{F}(\\mathbf{x}^k+\\mathbf{\\Delta x})\\|$ pode exceder a norma de $\\| \\mathbf{F}(\\mathbf{x}^k\\|$ de modo que nenhum progresso será feito na busca da solução no sistema não linear.\n",
    "\n",
    "O emprego de técnicas de globalização pode ser aplicada de forma a reduzir a norma de função residuo em cada iteração. Dessa forma, expandindo o domínio de convergência da iteração de Newton.\n",
    "\n",
    "O método de Busca linear (**Line search**) substitui a atualização original do método de Newton por:\n",
    "\n",
    "$$\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\lambda^{(k)}\\mathbf{\\Delta x} \\quad \\quad (1)$$\n",
    "\n",
    "onde $\\lambda^{(k)}$ é um numero real maior que zero. Em nossoas algoritmos até aqui aceitavamos $\\mathbf{\\Delta x}$ como a direção de busca para a solução do sistema não linear, mas na verdade, o que iremos fazer agora é usar um passo de tamanho reduzido $\\lambda^{(k)}<1$.\n",
    "\n",
    "**A questão é como escolher $\\lambda^{(k)}$?**\n",
    "\n",
    "Observe que para um sistema de equações lineares $\\mathbf{F(x)=0}$ não existe como determinar antecipadamente se um dado\n",
    "vetor será melhor ou pior para um aproximação que leva a solução exata. No entato, podemos introduzir uma função objetivo que nos de esse indicativo, por exemplo:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{F}(\\mathbf{x})\\|_2^2 \\quad \\quad (2)\n",
    "$$\n",
    "Para determinar $\\lambda^{(k)}$, basta resolver o problema de minimização\n",
    "\n",
    "$$\n",
    "\\min_{λ>0}ϕ(\\mathbf{x}+λ \\mathbf{\\Delta x}) \\quad \\quad (3)\n",
    "$$\n",
    "\n",
    "ou seja, pode-se tornar a função objetivo tão pequena quanto possível ao longo da direção de busca. No entanto, resolver (2) com precisão raramente é justificado uma vez que $\\lambda^{(k)}$ determina apenas a próxima iteração de Newton e não a resposta final.\n",
    "\n",
    "Assim, um algoritmo prático de busca linear testa valores candidatos de $\\lambda^k$ para verificar se $\\phi(\\mathbf{x}^k+\\lambda \\mathbf{s})$ é reduzido. Exigindo diminuição suficiente da função objetivo, temos:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}^k+\\lambda \\mathbf{\\Delta x}) \\le\n",
    "\\phi(\\mathbf{x}^k) + \\alpha\\lambda\\mathbf{\\Delta x}^T\\nabla\\phi(\\mathbf{x}^k) \\quad \\quad (3)\n",
    "$$\n",
    "\n",
    "com parametro $α∈(0,1)$ é uma forma comum de garantir o progresso. O valor padrão $α=10^{−4}$ corresponde a um critério de redução de norma fácil de satisfazer, mas essa busca pode falhar se (3) não for satisfeito após um certo número de reduções de $λ$ (pro exemplo, após 10 iterações).\n",
    "\n",
    "A quantidade $\\mathbf{\\Delta x}^T\\nabla\\phi(\\mathbf{x}^k)$ em (3) é a derivada direcional de $\\phi(\\mathbf{x}^k)$ na direção de busca $\\mathbf{\\Delta x}$. Para a função objetivo (2) isso é sempre negativo, se a equação  for resolvida com precisão:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Delta x}^T\\nabla\\phi(\\mathbf{x}^k)=\\mathbf{\\Delta x}^T\\mathbf{J}(\\mathbf{x}^k)^T\\mathbf{F}(\\mathbf{x}^k)=(\\mathbf{J}(\\mathbf{x}^k)\\mathbf{\\Delta x})^T\\mathbf{F}(\\mathbf{x}^k)=−\\|\\mathbf{F}(\\mathbf{x}^k)\\|_2^2 \\quad \\quad (4)\n",
    "$$\n",
    "\n",
    "\n",
    "Observe que inequaldade (3) pode ser testada sem a necessidade de avaliar o Jacobiano.\n",
    "\n",
    "O código abaixo implementa o algoritmos algoritmo de line search descrito no livro de Dennis and Schnabe, Numerical Methods for Nonlinear Equationsand Unconstrained Optimization\", Prentice-Hall (1983), seção 6.3.2.\n",
    "\n"
   ],
   "metadata": {
    "id": "S7A8jgucdhSH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def linesearch(X, S):\n",
    "    F = ComputeResiduo(X)\n",
    "    fnorm = norm(F)\n",
    "    XP = np.zeros([2, 1])\n",
    "    f = fnorm**2\n",
    "\n",
    "    slope = -fnorm * fnorm\n",
    "\n",
    "    if slope > 0:\n",
    "        initslope = -slope\n",
    "    if slope == 0:\n",
    "        slope = -1.0\n",
    "\n",
    "    lambda_min = 1.0e-08\n",
    "    lambda_ = 1.0\n",
    "    alpha_ = 1.0e-4\n",
    "    k = 0\n",
    "    kmax = 6\n",
    "    lambda_prev = 0\n",
    "    gprev = 0\n",
    "    while True:\n",
    "        # backtrack step\n",
    "        k = k + 1\n",
    "\n",
    "        # compute x + \\lamdda*s\n",
    "        XP = X + lambda_ * S\n",
    "\n",
    "        # Compute objective function\n",
    "        F = ComputeResiduo(XP)\n",
    "        gnorm = norm(F)\n",
    "        g = gnorm**2\n",
    "\n",
    "        if g <= (f + 2.0 * lambda_ * alpha_ * slope):\n",
    "            break\n",
    "        if k == kmax:\n",
    "            break\n",
    "        if lambda_ < lambda_min:\n",
    "            break\n",
    "        if k == 1:\n",
    "            # first backtracking, quadratic\n",
    "            lambda_temp = -slope / (g - f - 2.0 * slope)\n",
    "        else:\n",
    "            # all subsequent backtrackings, cubic fit\n",
    "            t1 = 0.5 * (g - f) - lambda_ * slope\n",
    "            t2 = 0.5 * (gprev - f) - lambda_prev * slope\n",
    "            t3 = 1.0 / (lambda_ - lambda_prev)\n",
    "            t4 = lambda_ * lambda_\n",
    "            t5 = lambda_prev * lambda_prev\n",
    "            a = t3 * (t1 / t4 - t2 / t5)\n",
    "            b = t3 * (lambda_ * t2 / t5 - lambda_prev * t1 / t4)\n",
    "            d = b * b - 3.0 * a * slope\n",
    "            if d < 0.0:\n",
    "                d = 0.0\n",
    "            if a == 0.0:\n",
    "                lambda_temp = -slope / (2.0 * b)\n",
    "            else:\n",
    "                lambda_temp = (-b + np.sqrt(d)) / (3.0 * a)\n",
    "\n",
    "        lambda_prev = lambda_\n",
    "        gprev = g\n",
    "\n",
    "        print(\" lambda_temp %14.12e\" % (lambda_temp))\n",
    "        if lambda_temp > 0.5 * lambda_:\n",
    "            lambda_ = 0.5 * lambda_\n",
    "        if lambda_temp <= 0.1 * lambda_:\n",
    "            lambda_ = 0.1 * lambda_\n",
    "        else:\n",
    "            lambda_ = lambda_temp\n",
    "\n",
    "    print(\"  Line search fnorm %12.12e - gnorm %14.12e\" % (fnorm, gnorm))\n",
    "    return lambda_\n",
    "\n",
    "\n",
    "def NewtonLS(x0, kmax, tolerance):\n",
    "    # print(\"%-5s  %-10.8s %-10.8s %-8.8s\" % (\"Iter\", \"x1\", \"x2\", \"error\") )\n",
    "    for i in range(kmax):\n",
    "        J = ComputeExJacobian(x0)\n",
    "        F = ComputeResiduo(x0)\n",
    "        dx = np.linalg.solve(J, -F)\n",
    "        lambda_ = linesearch(x0, dx)\n",
    "        x = x0 + lambda_ * dx\n",
    "        error = np.linalg.norm(x - x0) / np.linalg.norm(x)\n",
    "        # print(\"Newton Step: %d  -- Relative Error: %12.12e\" %(i,error))\n",
    "        print(\"%-5d  %-8.8f %-8.8f %-8.8E  %8.8E\" % (i, x[0], x[1], error, lambda_))\n",
    "        if error < tolerance:\n",
    "            break\n",
    "        x0 = x\n",
    "    return x\n",
    "\n",
    "\n",
    "x0 = np.zeros([2, 1])\n",
    "x0[0] = 10\n",
    "x0[1] = 10\n",
    "x = NewtonLS(x0, 50, 1.0e-8)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoxHFEIDkXeC",
    "outputId": "5fb104eb-f756-416c-f0a0-d17f24ffc316"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Line search fnorm 2.425825877050e+08 - gnorm 8.924115013397e+07\n",
      "0      9.50000000 0.55000000 9.94463012E-01  1.00000000E+00\n",
      "  Line search fnorm 8.924115013397e+07 - gnorm 3.283003072525e+07\n",
      "1      8.99999960 -72.22499318 9.99905611E-01  1.00000000E+00\n",
      "  Line search fnorm 3.283003072525e+07 - gnorm 1.207748922069e+07\n",
      "2      8.49999905 -35.62097710 9.99627103E-01  1.00000000E+00\n",
      "  Line search fnorm 1.207748922069e+07 - gnorm 4.443057569049e+06\n",
      "3      7.99999835 -16.92968771 9.98572913E-01  1.00000000E+00\n",
      "  Line search fnorm 4.443057569049e+06 - gnorm 1.634507629645e+06\n",
      "4      7.49999758 -6.84047949 9.95135785E-01  1.00000000E+00\n",
      "  Line search fnorm 1.634507629645e+06 - gnorm 6.012991934020e+05\n",
      "5      6.99999760 0.07001021 9.89744343E-01  1.00000000E+00\n",
      "  Line search fnorm 6.012991934020e+05 - gnorm 2.374159604496e+05\n",
      "6      6.49975417 -292.75485059 9.99994169E-01  1.00000000E+00\n",
      "  Line search fnorm 2.374159604496e+05 - gnorm 8.420613608367e+04\n",
      "7      5.99942328 -146.31808768 9.99976684E-01  1.00000000E+00\n",
      "  Line search fnorm 8.420613608367e+04 - gnorm 3.042585433018e+04\n",
      "8      5.49897387 -73.05998478 9.99906851E-01  1.00000000E+00\n",
      "  Line search fnorm 3.042585433018e+04 - gnorm 1.109566545062e+04\n",
      "9      4.99836522 -36.36757075 9.99628897E-01  1.00000000E+00\n",
      "  Line search fnorm 1.109566545062e+04 - gnorm 4.063937686394e+03\n",
      "10     4.49754886 -17.92287808 9.98534615E-01  1.00000000E+00\n",
      "  Line search fnorm 4.063937686394e+03 - gnorm 1.491202567470e+03\n",
      "11     3.99648842 -8.55076677 9.94371703E-01  1.00000000E+00\n",
      "  Line search fnorm 1.491202567470e+03 - gnorm 5.473232635523e+02\n",
      "12     3.49526070 -3.63417667 9.80136328E-01  1.00000000E+00\n",
      "  Line search fnorm 5.473232635523e+02 - gnorm 2.004710559509e+02\n",
      "13     2.99456530 -0.75540007 9.46127250E-01  1.00000000E+00\n",
      "  Line search fnorm 2.004710559509e+02 - gnorm 7.289372833298e+01\n",
      "14     2.50194018 2.94307353 9.65909985E-01  1.00000000E+00\n",
      "  Line search fnorm 7.289372833298e+01 - gnorm 2.707869951749e+01\n",
      "15     2.00863410 0.99732842 8.95077751E-01  1.00000000E+00\n",
      "  Line search fnorm 2.707869951749e+01 - gnorm 1.030648859972e+01\n",
      "16     1.50836138 -0.01514912 7.48674600E-01  1.00000000E+00\n",
      "  Line search fnorm 1.030648859972e+01 - gnorm 4.906596925410e+00\n",
      "17     1.10571174 1.98830781 8.98220109E-01  1.00000000E+00\n",
      "  Line search fnorm 4.906596925410e+00 - gnorm 1.321092207743e+00\n",
      "18     0.73129301 1.14639390 6.77620530E-01  1.00000000E+00\n",
      "  Line search fnorm 1.321092207743e+00 - gnorm 3.039041379317e-01\n",
      "19     0.45229172 0.95407560 3.20937188E-01  1.00000000E+00\n",
      "  Line search fnorm 3.039041379317e-01 - gnorm 3.362799653044e-02\n",
      "20     0.33631301 0.94887914 1.15320521E-01  1.00000000E+00\n",
      "  Line search fnorm 3.362799653044e-02 - gnorm 5.862535028657e-04\n",
      "21     0.31992836 0.94758428 1.64335142E-02  1.00000000E+00\n",
      "  Line search fnorm 5.862535028657e-04 - gnorm 1.895681417013e-07\n",
      "22     0.31963163 0.94754193 2.99737899E-04  1.00000000E+00\n",
      "  Line search fnorm 1.895681417013e-07 - gnorm 1.986647859192e-14\n",
      "23     0.31963154 0.94754191 9.71783244E-08  1.00000000E+00\n",
      "  Line search fnorm 1.986647859192e-14 - gnorm 1.570092458684e-16\n",
      "24     0.31963154 0.94754191 1.01670306E-14  1.00000000E+00\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Referências\n",
    "\n",
    "1. J. E. Dennis, Jr. and Robert B. Schnabel, Numerical Methods for Unconstrained Optimization and Nonlinear Equations, 1996. Book Series Name:Classics in Applied Mathematics. SIAM.\n",
    "2. C. T. Kelley.Iterative Methods for Linear and Nonlinear Equations, 1995, Book Series Name:Frontiers in Applied Mathematics. SIAM.\n",
    "3. Jorge Nocedal, Stephen J. Wright, Numerical Optimization, 2006, Springer New York, NY."
   ],
   "metadata": {
    "id": "qYsuHzPtgQSo"
   }
  }
 ]
}